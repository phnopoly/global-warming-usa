{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the librarys needed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from numpy import mean\n",
    "from numpy import std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run previous ipynb files First then call MAIN_GRID\n",
    "%run extractData.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAIN_GRID[1][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = MAIN_GRID[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0.info() #checking the data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df0.columns\n",
    "df0 = df0.rename(index=str, columns={'CSIRO - Adjusted sea level (inches)': \"CSIRO_ASLinches\", 'Year':'Year_Merge'}) #Rename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter data set to 1959-2015 and the columns Year and CSIRO_ASLinches\n",
    "df0_filter = df0.iloc[0:,[0,1]]\n",
    "df0_filter = (df0_filter.set_index(['Year_Merge']).loc[1959:2015]).reset_index()\n",
    "df0_filter.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking for null/na valfues if any exists\n",
    "df0_filter.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#towardsdatascience.com/handling-missing-values-with-pandas-b876bf6f008f\n",
    "#Filling 2 null values with mean of the records \n",
    "#df0_mean = df0_filter['CSIRO_ASLinches'].mean()\n",
    "#df0_filter['CSIRO_ASLinches'].fillna(df0_mean)\n",
    "#df0_mean #6.199642083854547, which is less than that of NOAA have recorded for these years\n",
    "#Alternatively missing values can be replaced with the values before or after them.\n",
    "df0_filter = df0_filter.fillna(axis=0, method = 'ffill', limit=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Anamoly Detection #https://help.ceda.ac.uk/article/4728-cru-data-python-example\n",
    "avCSIRO_ASLinches_1959_2015 = np.mean(df0_filter['CSIRO_ASLinches']) #Calculating 57 year average\n",
    "df0_filter['Anamoly_CSIRO_ASLinches'] = df0_filter['CSIRO_ASLinches'] - avCSIRO_ASLinches_1959_2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://datahub.io/core/global-temp #useful reference data source\n",
    "\n",
    "df1 = MAIN_GRID[0][0]\n",
    "df1.info() #checking the data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns #checking the column names if any whitespaces exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename column name\n",
    "df1 = df1.rename(index=str, columns={'Rainfall - (MM)': \"Rnf_MM\", \" Year\": \"Year\", })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter data set to 1959-2015 and the columns Year and Rainfall (MM)\n",
    "df1_filter = df1.iloc[0:,[0,1]]\n",
    "df1_filter = (df1_filter.set_index(['Year']).loc[1959:2015]).reset_index()\n",
    "df1_filter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Year to datatime format\n",
    "df1_filter['Year'] = pd.to_datetime(df1_filter['Year'], format='%Y')\n",
    "#Convert Year column  to index\n",
    "df1_filter.set_index('Year',inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternative methods GroupBy and plot\n",
    "df1_filter_averagegrpby = df1_filter.groupby(pd.Grouper(freq='Y')).mean() #.plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Anamoly Detection #https://help.ceda.ac.uk/article/4728-cru-data-python-example\n",
    "\n",
    "avprecp_1959_2015 = np.mean(df1_filter_averagegrpby['Rnf_MM']) #Calculating 57 year average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_filter_averagegrpby['Anamoly_RnfMM'] = df1_filter_averagegrpby['Rnf_MM'] - avprecp_1959_2015\n",
    "df1_filter_averagegrpby.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1_filter_averagegrpby.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove index column\n",
    "#df1_filter_averagegrpby.index #removes time portion from time stamp\n",
    "df1_filter_averagegrpby.reset_index(level=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract Year to Year_Merge Column\n",
    "df1_filter_averagegrpby['Year_Merge'] = pd.DatetimeIndex(df1_filter_averagegrpby['Year']).year\n",
    "df1_filter_averagegrpby.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = MAIN_GRID[0][1]\n",
    "df2.info() #checking the data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns\n",
    "#df2_filter.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename column name\n",
    "df2 = df2.rename(index=str, columns={'Temperature - (Celsius)':\"Tmp_Cls\",\" Year\": \"Year\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter data set to 1959-2015 and the columns Year and Rainfall (MM)\n",
    "df2_filter = df2.iloc[0:,[0,1]]\n",
    "df2_filter = (df2_filter.set_index(['Year']).loc[1959:2015]).reset_index()\n",
    "df2_filter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Year to datatime format\n",
    "#from io import StringIO\n",
    "#df2_average = pd.read_csv(StringIO(df2_filter),header=True,#parse_dates=['Year'],index_col='Year')\n",
    "df2_filter['Year'] = pd.to_datetime(df2_filter['Year'], format='%Y')\n",
    "\n",
    "#df2_filter['Year'] = pd.to_datetime(df2_filter['Year'])\n",
    "#df2_filter['Year'] = df2_filter['Year'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert Year column  to index\n",
    "df2_filter.set_index('Year',inplace=True)\n",
    "#df2_filter.info()\n",
    "df2_filter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/23859840/python-aggregate-by-month-and-calculate-average\n",
    "#df2_filter_average = df2_filter.resample('Y').mean()\n",
    "#Adding a year counter\n",
    "#df2_filter_average['cnt'] = range(len(df2_filter_average))\n",
    "#df2_filter_average\n",
    "#Alternative methods GroupBy and plot\n",
    "df2_filter_averagegrpby = df2_filter.groupby(pd.Grouper(freq='Y')).mean()#.plot()\n",
    "df2_filter_averagegrpby.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Anamoly Detection #https://help.ceda.ac.uk/article/4728-cru-data-python-example\n",
    "import numpy as np\n",
    "av_1959_2015 = np.mean(df2_filter_averagegrpby['Tmp_Cls']) #Calculating 57 years average\n",
    "df2_filter_averagegrpby['Anamoly_TmpCls'] =df2_filter_averagegrpby['Tmp_Cls'] - av_1959_2015\n",
    "df2_filter_averagegrpby.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove index column\n",
    "#df1_filter_averagegrpby.index #removes time portion from time stamp\n",
    "df2_filter_averagegrpby.reset_index(level=0, inplace=True)\n",
    "#Extract Year to Year_Merge Column\n",
    "df2_filter_averagegrpby['Year_Merge'] = pd.DatetimeIndex(df1_filter_averagegrpby['Year']).year\n",
    "df2_filter_averagegrpby.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = MAIN_GRID[2][0]\n",
    "df3.info() #checking the data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##checking the column names to identify white spaces if any\n",
    "df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename column name\n",
    "df3 = df3.rename(index=str, columns={\"Year (negative values = BC)\": \"Year\", \"Mauna Loa, Hawaii\": \"MLHawai_CO2ppm\"})\n",
    "#Filter data set to columns Year and ml_hawai\n",
    "df3_filter = df3.iloc[0:,[0,4]]\n",
    "#convert everything to numerical values #https://stackoverflow.com/questions/47444999/check-if-column-contains-type-string-object\n",
    "df3_filter.loc[:, df3_filter.dtypes.eq('object')] = df3_filter.loc[:, df3_filter.dtypes.eq('object')].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "#checking the data types\n",
    "df3_filter.info()\n",
    "\n",
    "#df3_filter.isnull().sum() #Checking if any null value exists.\n",
    "#df3_filter.isna().sum() #Checking if any na value exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filter data set to 1959-2015 and the columns Year and ml_hawai\n",
    "df3_filter = (df3_filter.set_index(['Year']).loc[1959:2015]).reset_index()\n",
    "df3_filter.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking if any null value exists.\n",
    "df3_filter.isna().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting year to int64\n",
    "#df3_filter['Year'] = df3_filter['Year'].astype(np.int64) #casting float64 to int64\n",
    "#Convert Year to datatime format\n",
    "df3_filter['Year'] = pd.to_datetime(df3_filter['Year'], format='%Y')\n",
    "#Convert Year column  to index\n",
    "#df3_filter.set_index('Year',inplace=True)\n",
    "df3_filter.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Anamoly Detection #https://help.ceda.ac.uk/article/4728-cru-data-python-example\n",
    "avCO2ppm_1959_2015 = np.mean(df3_filter['MLHawai_CO2ppm']) #Calculating 57 year average\n",
    "df3_filter['Anamoly_CO2ppm'] =df3_filter['MLHawai_CO2ppm'] - avCO2ppm_1959_2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove index column\n",
    "#df1_filter_averagegrpby.index #removes time portion from time stamp\n",
    "#df1_filter_averagegrpby.reset_index(level=0, inplace=True)\n",
    "#Extract Year to Year_Merge Column\n",
    "df3_filter['Year_Merge'] = pd.DatetimeIndex(df3_filter['Year']).year\n",
    "df3_filter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dataframes and fill the values that don't exist in the lines of merged dataframe simply fill with required strings as #https://stackoverflow.com/questions/44327999/python-pandas-merge-multiple-dataframes#:~:text=Just%20simply%20merge%20with%20DATE,to%20get%20all%20the%20data).&text=Now%2C%20basically%20load%20all%20the,using%20merge%20or%20reduce%20function.&text=Note%3A%20you%20can%20add%20as,frames%20inside%20the%20above%20list.\n",
    "\n",
    "from functools import reduce\n",
    "df_tobe_merged = [df1_filter_averagegrpby,df2_filter_averagegrpby,df0_filter,df3_filter]\n",
    "df_merged = reduce(lambda  left,right: pd.merge(left,right,on=['Year_Merge'], how='outer'), df_tobe_merged).fillna('void')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_merged.info()\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rearrange the columns to the dataframe merged\n",
    "df_merged_final = df_merged.iloc[0:,[3,1,2,5,6,7,8,10,11]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize\n",
    "from statistics import median \n",
    "print('mean=%.3f median =%.3f stdv=%.3f' % (mean(df1_filter[\"Rnf_MM\"]), median(df1_filter[\"Rnf_MM\"]), std(df1_filter[\"Rnf_MM\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normality Check # Hypothesis testing\n",
    "from scipy import stats\n",
    "k2, p = stats.normaltest(df1_filter[\"Rnf_MM\"])\n",
    "alpha = 1e-3\n",
    "print(\"p = {:g}\".format(p))\n",
    "\n",
    "if p < alpha:  # null hypothesis: x comes from a normal distribution\n",
    "    print(\"The null hypothesis can be rejected\")\n",
    "else:\n",
    "    print(\"The null hypothesis cannot be rejected\")\n",
    "#The null hypothesis cannot be rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normality Check # Hypothesis testing\n",
    "from scipy import stats\n",
    "k2, p = stats.normaltest(df3_filter[\"MLHawai_CO2ppm\"])\n",
    "alpha = 1e-3\n",
    "print(\"p = {:g}\".format(p))\n",
    "\n",
    "if p < alpha:  # null hypothesis: x comes from a normal distribution\n",
    "    print(\"The null hypothesis can be rejected\")\n",
    "else:\n",
    "    print(\"The null hypothesis cannot be rejected\")\n",
    "#The null hypothesis cannot be rejected, since the dependent variable is normally distributed, we can perform Pearson's Correlation Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html\n",
    "# calculate Pearson's correlation # Alternatively we can calculate (r) as \n",
    "#from scipy.stats import pearsonr\n",
    "#corr, _ = pearsonr(df_merged_final[\"Rnf_MM\"],df_merged_final[\"MLHawai_CO2ppm\"])\n",
    "#print('Pearsons correlation: %.3f' % corr)\n",
    "#scipy.stats.pearsonr\n",
    "#########################Alternative methods\n",
    "#from scipy import stats\n",
    "#print(stats.pearsonr(df_merged_final[\"Tmp_Cls\"],df_merged_final[\"MLHawai_CO2ppm\"]),\n",
    "    #stats.pearsonr(df_merged_final[\"Rnf_MM\"],df_merged_final[\"MLHawai_CO2ppm\"]),\n",
    "    #stats.pearsonr(df_merged_final[\"CSIRO_ASLinches\"],df_merged_final[\"MLHawai_CO2ppm\"]))\n",
    "# returns (r, p-value)\n",
    "\n",
    "#Pearson's Correlation Test\n",
    "#Null Hypothesis        : H0 : ρ0 = ρ1\n",
    "#Alternative hypothesis : H1 : ρ0 ≠ ρ1\n",
    "########################\n",
    "\n",
    "#https://www.kaggle.com/derevirn/data-analysis-on-climate-change\n",
    "#Check the correlations between all the attributes/  Anamoly and the Co2 Emmission\n",
    "#df_merged_final.columns #Printing out the column names\n",
    "df_correlation = df_merged_final[[\"Tmp_Cls\", \"Anamoly_TmpCls\",'Rnf_MM', 'Anamoly_RnfMM','CSIRO_ASLinches', 'Anamoly_CSIRO_ASLinches', \"MLHawai_CO2ppm\"]].copy()\n",
    "df_correlation.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Covariance Matrix #machinelearningmastery.com/how-to-use-correlation-to-understand-the-relationship-between-variables/\n",
    "from numpy import cov\n",
    "covariance = cov(df_merged_final[\"CSIRO_ASLinches\"], df_merged_final[\"MLHawai_CO2ppm\"])\n",
    "print(covariance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LINEAR REGRESSION TESTS##"
   ]
  },
  {
   "source": [
    "#Import the dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "#df= pd.read_csv('mergedgwd.csv')#,usecols=[1], engine='python')#, index_col='Name')\n",
    "#Convert Year to datatime format\n",
    "#df['Year_Merge'] = pd.to_datetime(df['Year_Merge'], format='%Y')\n",
    "#Convert Year column  to index\n",
    "#df.set_index('Year_Merge',inplace=True)\n",
    "df = df_merged_final"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Pearson correlation coefficient (r) is calculated to check the linearity, that we have assumed between df's independent variables.\n",
    "# Correlation Evaluations:\n",
    "#0.8-1.0(Very Strong), 0.6-0.8 (Strong), 0.4-0.6(Moderate), 0.2-0.4(Weak), 0.0-0.2(Very Weak)\n",
    "\n",
    "df.corr()[['Tmp_Cls']].sort_values('Tmp_Cls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting MLHawai_CO2ppm and CSIRO_ASLinches to the linear regression model since theres the strong correlation with Tmp_Cls \n",
    "predictors = ['CSIRO_ASLinches','MLHawai_CO2ppm']\n",
    "df_LRM = df[['Tmp_Cls']+predictors]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vizualize the correlation with scatterplot\n",
    "#Scatter Plot # Distribution\n",
    "import altair as alt\n",
    "from vega_datasets import data\n",
    "\n",
    "alt.Chart(df_LRM).mark_circle().encode(\n",
    "    alt.X(alt.repeat(\"column\"), type='quantitative'),\n",
    "    alt.Y(alt.repeat(\"row\"), type='quantitative'),\n",
    "    color='Year_Merge:O'\n",
    ").properties(\n",
    "    width=100,\n",
    "    height=100\n",
    ").repeat(\n",
    "    column=['Tmp_Cls','CSIRO_ASLinches','MLHawai_CO2ppm'],\n",
    "    row=['MLHawai_CO2ppm','CSIRO_ASLinches','Tmp_Cls']\n",
    ")#.interactive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler() \n",
    "scaled_values = scaler.fit_transform(df_LRM) \n",
    "df_LRM.loc[:,:] = scaled_values\n",
    "df_LRM.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Response variable Tmp_Cls shows the strong relation ship with predictor variables MLHawai_CO2ppm and CSIRO_ASLinches\n",
    "#evaluate the significance of each of the included predictor variables.\n",
    "\n",
    "#H0: βj = 0, the null hypothesis states that the predictor has no effect on the outcome variable's value\n",
    "#Ha: βj ≠ 0, the alternative hypothesis is that the predictor has a significant effect on the outcome variable's value\n",
    "\n",
    "# import the relevant module\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# separate our my predictor variables (X) from our outcome variable y\n",
    "X = df_LRM[predictors]\n",
    "y = df_LRM['Tmp_Cls']\n",
    "\n",
    "# Add a constant to the predictor variable set to represent the Bo intercept\n",
    "X = sm.add_constant(X)\n",
    "X.iloc[:3, :3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply Sequential Backward Selection for the best set of features to predict model\n",
    "\n",
    "# (1) select a significance value\n",
    "alpha = 0.05\n",
    "\n",
    "# (2) Fit the model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# (3) evaluate the coefficients' p-values\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate p-values and remove the least significant value\n",
    "#Above case since the CSIRO_ASLinches is less significant or p value is greater than 0.05 or accept the null hypothesis that the predictor has no effect on the outcome variable's value\n",
    "\n",
    "# (3) cont. - Identify the predictor with the greatest p-value and assess if its > our selected alpha.\n",
    "#             based off the table it is clear that CSIRO_ASLinches has the greatest p-value and that it is\n",
    "#             greater than our alpha of 0.05\n",
    "\n",
    "# (4) - Use pandas drop function to remove this column from X\n",
    "\n",
    "#from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = X.drop('CSIRO_ASLinches', axis=1)\n",
    "\n",
    "# (5) Fit the model \n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#p value is less than ALpha or  0.05\n",
    "#1) the R-squared and Adj. R-squared values are almost equal which suggests there is minimal risk that our model \n",
    "#is being over fitted by excessive variables and \n",
    "#(2) the value of 0.57 is interpreted such that our final model explains about 60% of the observed variation \n",
    "#in the outcome variable, the \"meantempm\".\n",
    "\n",
    "#NEXT STEP# FORECASTING\n",
    "\n",
    "#forecasting for next 30 years\n",
    "#Liear Regression model for scikit learn.org#scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html\n",
    "df_LRM = df[['Tmp_Cls']+predictors]\n",
    "df_LRM.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the data\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler() \n",
    "scaled_values = scaler.fit_transform(df_LRM) \n",
    "df_LRM.loc[:,:] = scaled_values\n",
    "df_LRM.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate our my predictor variables (X) from our outcome variable y\n",
    "X = df_LRM[predictors]\n",
    "y = df_LRM['Tmp_Cls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stackabuse.com/linear-regression-in-python-with-scikit-learn/\n",
    "#SciKit-Learn's LinearRegression Module to Predict the Weather\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "# instantiate the regressor class\n",
    "regressor = LinearRegression()\n",
    "\n",
    "# fit the build the model by fitting the regressor to the training data\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# make a prediction set using the test set\n",
    "prediction = regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the prediction accuracy of the model\n",
    "from sklearn.metrics import mean_absolute_error, median_absolute_error\n",
    "print(\"The Explained Variance: %.2f\" % regressor.score(X_test, y_test))\n",
    "print(\"The Mean Absolute Error: %.2f degrees celsius\" % mean_absolute_error(y_test, prediction))\n",
    "print(\"The Median Absolute Error: %.2f degrees celsius\" % median_absolute_error(y_test, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model is able to explain about 60% of the variance observed in the response variable Tmp_Cls\n",
    "#mean_absolute_error() and median_absolute_error() of the sklearn.metrics module to determine that on average the \n",
    "#predicted value is about 0.12 degrees Celsius off and half of the time it is off by about 0.10 degrees Celsius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction\n",
    "#prediction_OLS = (start = len(df_LRM),  \n",
    "#                          end = (len(df_LRM)-1) + 10 ,  \n",
    "#                          typ = 'levels').rename('Forecast') \n",
    "\n",
    "prediction_OLS = model.predict(X)\n",
    "prediction_OLS.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARIMA-Autoregressive integrated moving average model for non seasonal temperature data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARIMA MODEL\n",
    "df = df_merged_final\n",
    "\n",
    "#df_arima.plot()\n",
    "#plt.show()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subsample the data set\n",
    "df_arima =df[['Year_Merge','Tmp_Cls']]\n",
    "\n",
    "#Convert Year to datatime format\n",
    "df_arima['Year_Merge'] = pd.to_datetime(df_arima['Year_Merge'], format='%Y')\n",
    "#Convert Year column  to index\n",
    "df_arima.set_index('Year_Merge',inplace=True)\n",
    "#df_arima.index = pd.DatetimeIndex(df_arima.index).to_period('Y')#set the frequency to avoid error #('M')\n",
    "print(df_arima.index,df_arima.info(), df_arima.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)\n",
    "#ARIMA Model\n",
    "#https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/\n",
    "#y_hat_avg = test.copy()\n",
    "#fit1 = sm.tsa.statespace.SARIMAX(X_train., order=(2, 1, 4),seasonal_order=(0,1,1,7)).fit()\n",
    "\n",
    "#Normalize the data\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler() \n",
    "scaled_values = scaler.fit_transform(df_arima) \n",
    "df_arima.loc[:,:] = scaled_values\n",
    "\n",
    "df_arima.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_arima.plot(color='r')\n",
    "plt.grid()\n",
    "plt.title('Temperature Trend over the time')\n",
    "plt.ylabel(\"Normalized Annual Mean Temperature\")\n",
    "#fit function\n",
    "#a, b = np.polyfit(np.array(df_arima['Year_Merge']),np.aray(df_arima['Tmp_Cls']), deg=1)\n",
    "#f = lambda x: a*x + b\n",
    "#plt.rcParams['figure.figsize'] = (20, 10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from matplotlib import pyplot\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "autocorrelation_plot(df_arima)\n",
    "plt.show()\n",
    "\n",
    "#Running the example, we can see that there is a positive correlation with the first 10-to-15 lags that is perhaps significant for the first 10 lags.\n",
    "\n",
    "#A good starting point for the AR parameter of the model may be 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lag value to 10 for autoregression, uses a difference order of 1 to make the time series stationary, and uses a moving average\n",
    "#model of 0.\n",
    "\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from pandas import DataFrame\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# fit model ARIMA\n",
    "model = ARIMA(df_arima, order=(10,1,0)) #p, d, and q parameters\n",
    "model_fit = model.fit(disp=0) #When fitting the model, a lot of debug information is provided about the fit of the linear regression model. We can turn this off by setting the disp argument to 0.\n",
    "print(model_fit.summary())\n",
    "\n",
    "# plot residual errors\n",
    "residuals = DataFrame(model_fit.resid)\n",
    "residuals.plot()\n",
    "pyplot.show()\n",
    "residuals.plot(kind='kde')\n",
    "pyplot.show()\n",
    "print(residuals.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The distribution of the residual errors is displayed. The results show that indeed there is a bias in the prediction\n",
    "#(a non-zero mean in the residuals).\n",
    "\n",
    "#Note, that although above we used the entire dataset for time series analysis, ideally we would perform this analysis on just the training dataset when developing a predictive model.\n",
    "#Next, let’s look at how we can use the ARIMA model to make forecasts.\n",
    "\n",
    "#Rolling Forecast ARIMA Model\n",
    "\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from matplotlib import pyplot\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from sklearn.metrics import mean_squared_error\n",
    " \n",
    "#def parser(x):\n",
    "    #return datetime.strptime('190'+x, '%Y-%m')\n",
    " \n",
    "#series = read_csv('dfmerged.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n",
    "X = df_arima.values\n",
    "size = int(len(X) * 0.66)\n",
    "train, test = X[0:size], X[size:len(X)]\n",
    "history = [x for x in train]\n",
    "predictions = list()\n",
    "for t in range(len(test)):\n",
    "    model = ARIMA(history, order=(2,1,1)) #(2,1,1) suggested from HyperParameters Tuning\n",
    "    model_fit = model.fit(disp=0)\n",
    "    output = model_fit.forecast()\n",
    "    yhat = output[0]\n",
    "    predictions.append(yhat)\n",
    "    obs = test[t]\n",
    "    history.append(obs)\n",
    "    print('predicted=%f, expected=%f' % (yhat, obs))\n",
    "error = mean_squared_error(test, predictions)\n",
    "print('Test MSE: %.3f' % error)\n",
    "# plot\n",
    "#plt.plot(test)\n",
    "#plt.plot(predictions, color='red')\n",
    "#plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot #https://matplotlib.org/tutorials/intermediate/legend_guide.html\n",
    "#Plot the test vs predicted values for the evaluation of model performance.\n",
    "plt.title(\"Training model Performance\") \n",
    "plt.xlabel(\"Length\") \n",
    "plt.ylabel(\"Noramalized Temperature Scale\") \n",
    "plt.plot(test, label = 'Test Value', linestyle='--')\n",
    "plt.plot(predictions, color='red', label ='Prdeicted value')\n",
    "# Place a legend to the right of this smaller subplot.\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "plt.show()\n",
    "import matplotlib\n",
    "matplotlib.rc('figure', figsize=[20,10])\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 15}\n",
    "\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stackoverflow.com/questions/45596492/statsmodels-arima-different-results-using-predict-and-forecast\n",
    "#df_future =  model_fit.forecast(steps = 20)\n",
    "#df_future\n",
    "df_arima =df[['Year_Merge','Tmp_Cls']] #switiching back to original dataframe without normalization for actual value.\n",
    "# Forecast for the next 30 years \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX \n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "#Forecast using ARIMA Model\n",
    "# Train the model on the full dataset \n",
    "modelarima = ARIMA(df_arima[\"Tmp_Cls\"],  \n",
    "                        order =(2, 1, 1)) #(3, 2, 1))  gave best forecast however\n",
    "                        ##,seasonal_order =(2, 1, 1, 12)) \n",
    "fits = modelarima.fit() \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "forecast_1 =  fits.forecast(steps = 50)[0] #[0]#(start = len(df_arima),  \n",
    "                          #end = (len(df_arima)-1) + 30,  \n",
    "                          #typ = 'levels').rename('Forecast') \n",
    "print(forecast_1) #Check the prdicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rf =  pd.to_datetime(pd.date_range('2016-01-01', periods=30, freq='AS'))\n",
    "#rf\n",
    "import pandas as pd\n",
    "##convert ndarray to Pandas Series\n",
    "index =[pd.date_range('2016-01-01', periods=50, freq='AS')]#create index for the forecasted date range\n",
    "#columns = ['ForcasetTMP_Cls']\n",
    "name = 'ForecastedTemp'\n",
    "forecasted = pd.Series(data = forecast_1, index=index,  name=name, dtype = 'float').to_frame() #convert ndarray to Pandas Series to data frame.\n",
    "#forecasted=forecasted.to_frame() #series object to pd frame\n",
    "forecasted = forecasted.reset_index() #reset index to convert pandas dataframe format\n",
    "forecasted = forecasted.rename(columns={\"level_0\":\"Year_Merge\"},index={0: 0}) #rename index column\n",
    "#Convert Year to datatime format\n",
    "forecasted['Year_Merge'] = pd.to_datetime(forecasted['Year_Merge'], format='%Y')\n",
    "#Convert Year column  to index\n",
    "forecasted.set_index('Year_Merge',inplace=True)\n",
    "forecasted.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Subsample the data set\n",
    "df_arima =df[['Year_Merge','Tmp_Cls']]\n",
    "\n",
    "#Convert Year to datatime format\n",
    "df_arima['Year_Merge'] = pd.to_datetime(df_arima['Year_Merge'], format='%Y')\n",
    "#Convert Year column  to index\n",
    "df_arima.set_index('Year_Merge',inplace=True)\n",
    "#df_arima.index = pd.DatetimeIndex(df_arima.index).to_period('Y')#set the frequency to avoid error #('M')\n",
    "#print(df_arima.index,df_arima.info(), df_arima.head(3))\n",
    "\n",
    "\n",
    "import altair as alt\n",
    "from vega_datasets import data\n",
    "#alt.data_transformers.enable('use_index') #to reset the index #https://github.com/altair-viz/altair/issues/271\n",
    "\n",
    "Chart1 = alt.Chart(df_arima.reset_index()).mark_line().encode(\n",
    "    x='Year_Merge',\n",
    "    y='Tmp_Cls',\n",
    "    #color=alt.value(\"#FFAA00\")\n",
    " \n",
    "    \n",
    "    )\n",
    "Chart2 =alt.Chart(forecasted.reset_index()).mark_line().encode(\n",
    "    x='Year_Merge',\n",
    "    y='ForecastedTemp',\n",
    "    color=alt.value(\"#FFAA00\")\n",
    "    \n",
    "    )\n",
    "(Chart1 +Chart2).properties(title='Future Prediction', width=500)\n",
    "#Chart(data.reset_index()).mark_line().encode(\n",
    "    #x='index',\n",
    "    #y='value'\n",
    "#)\n",
    "#df_arima.plot(color='r')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}